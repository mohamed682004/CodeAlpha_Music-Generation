{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 21:12:34.133895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737400354.172807     405 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737400354.184158     405 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-20 21:12:34.226524: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import glob\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_midi_files(dataset_path):\n",
    "    \"\"\"Load and process MIDI files from the given path.\"\"\"\n",
    "    notes = []\n",
    "    \n",
    "    # Get all MIDI files\n",
    "    midi_files = glob.glob(os.path.join(dataset_path, \"*.midi\"))\n",
    "    print(f\"Processing {len(midi_files)} MIDI files...\")\n",
    "    \n",
    "    # Process each file\n",
    "    for i, file in enumerate(midi_files):\n",
    "        try:\n",
    "            print(f\"Processing file {i+1}/{len(midi_files)}: {os.path.basename(file)}\")\n",
    "            midi = converter.parse(file)\n",
    "            notes_to_parse = None\n",
    "            \n",
    "            try:\n",
    "                # Try to get the piano part\n",
    "                s2 = instrument.partitionByInstrument(midi)\n",
    "                if s2:  # If there are instrument parts\n",
    "                    notes_to_parse = s2.parts[0].recurse()\n",
    "                else:  # If there are no instrument parts\n",
    "                    notes_to_parse = midi.flat.notes\n",
    "            except Exception:\n",
    "                # If partitioning fails, use flat representation\n",
    "                notes_to_parse = midi.flat.notes\n",
    "            \n",
    "            # Extract notes and chords\n",
    "            for element in notes_to_parse:\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    print(f\"Successfully extracted {len(notes)} notes and chords\")\n",
    "    return notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare the input sequences and output labels\n",
    "def prepare_sequences(notes, n_vocab, sequence_length=100):\n",
    "    pitchnames = sorted(set(notes))\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    network_input = network_input / float(n_vocab)  # Normalize input\n",
    "    network_output = to_categorical(network_output)  # One-hot encode output\n",
    "\n",
    "    return network_input, network_output, note_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, n_vocab):\n",
    "    model = Sequential([\n",
    "        LSTM(64, input_shape=input_shape, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64),\n",
    "        Dense(n_vocab, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train the model\n",
    "def train_model(model, network_input, network_output, epochs=10, batch_size=32):\n",
    "    model.fit(network_input, network_output, epochs=epochs, batch_size=batch_size,validation_split=0.1,verbose=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(model, network_input, pitchnames, n_vocab, sequence_length=50, num_notes=200):\n",
    "    \"\"\"\n",
    "    Generate music with correct array shape handling\n",
    "    \"\"\"\n",
    "    int_to_note = dict(enumerate(pitchnames))\n",
    "    \n",
    "    # Get random starting seed\n",
    "    start = np.random.randint(0, len(network_input) - 1)\n",
    "    pattern = network_input[start].flatten()  # Flatten to 1D array\n",
    "    prediction_output = []\n",
    "\n",
    "    # Pre-allocate normalized input array\n",
    "    prediction_input = np.zeros((1, sequence_length, 1))\n",
    "    scale_factor = float(n_vocab)\n",
    "\n",
    "    for _ in range(num_notes):\n",
    "        # Reshape pattern correctly\n",
    "        prediction_input[0, :, 0] = pattern.flatten() / scale_factor\n",
    "\n",
    "        # Get prediction\n",
    "        index = np.argmax(model.predict(prediction_input, verbose=0))\n",
    "        prediction_output.append(int_to_note[index])\n",
    "\n",
    "        # Update pattern\n",
    "        pattern = np.roll(pattern, -1)\n",
    "        pattern[-1] = index\n",
    "\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import instrument, note, chord, stream, tempo\n",
    "\n",
    "def create_midi(prediction_output, filename=\"output.mid\", instrument_type=instrument.Piano, min_duration_secs=10):\n",
    "    \"\"\"\n",
    "    Create a MIDI file with a minimum duration of specified seconds\n",
    "    Args:\n",
    "        prediction_output: List of predicted notes/chords\n",
    "        filename: Output MIDI filename\n",
    "        instrument_type: MIDI instrument to use\n",
    "        min_duration_secs: Minimum duration in seconds\n",
    "    \"\"\"\n",
    "    # Create instrument instance once\n",
    "    piano = instrument_type()\n",
    "    output_notes = []\n",
    "    \n",
    "    # Calculate appropriate note duration\n",
    "    total_notes = len(prediction_output)\n",
    "    base_duration = min_duration_secs / (total_notes * 0.5)\n",
    "    note_duration = max(base_duration, 0.25)\n",
    "    \n",
    "    # Add tempo marking for consistent playback\n",
    "    tempo_mark = tempo.MetronomeMark(number=120)  # 120 BPM\n",
    "    output_notes.append(tempo_mark)\n",
    "    \n",
    "    current_time = 0.0\n",
    "    \n",
    "    for i, pattern in enumerate(prediction_output):\n",
    "        # Handle chords\n",
    "        if ('.' in pattern):\n",
    "            chord_notes = [\n",
    "                note.Note(\n",
    "                    int(current_note),\n",
    "                    quarterLength=note_duration,\n",
    "                    storedInstrument=piano\n",
    "                )\n",
    "                for current_note in pattern.split('.')\n",
    "                if current_note.isdigit()\n",
    "            ]\n",
    "            if chord_notes:\n",
    "                new_chord = chord.Chord(chord_notes)\n",
    "                new_chord.offset = current_time\n",
    "                output_notes.append(new_chord)\n",
    "        \n",
    "        # Handle single notes\n",
    "        elif not isinstance(pattern, str) or pattern.isdigit():\n",
    "            new_note = note.Note(\n",
    "                int(pattern) if isinstance(pattern, str) else pattern,\n",
    "                quarterLength=note_duration,\n",
    "                storedInstrument=piano\n",
    "            )\n",
    "            new_note.offset = current_time\n",
    "            output_notes.append(new_note)\n",
    "        \n",
    "        # Update timing\n",
    "        current_time += note_duration\n",
    "    \n",
    "    # Create and write stream\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    \n",
    "    # Calculate actual duration\n",
    "    actual_duration = current_time * 0.5  # Convert to seconds\n",
    "    print(f\"Created piece with duration: {actual_duration:.2f} seconds\")\n",
    "    \n",
    "    midi_stream.write('midi', fp=filename)\n",
    "    return actual_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 13 MIDI files...\n",
      "Processing file 1/13: MIDI-Unprocessed_01_R1_2008_01-04_ORIG_MID--AUDIO_01_R1_2008_wav--2.midi\n",
      "Processing file 2/13: MIDI-Unprocessed_XP_15_R2_2004_01_ORIG_MID--AUDIO_15_R2_2004_03_Track03_wav.midi\n",
      "Processing file 3/13: MIDI-Unprocessed_01_R1_2008_01-04_ORIG_MID--AUDIO_01_R1_2008_wav--1.midi\n",
      "Processing file 4/13: MIDI-Unprocessed_01_R1_2009_01-04_ORIG_MID--AUDIO_01_R1_2009_01_R1_2009_02_WAV.midi\n",
      "Processing file 5/13: MIDI-Unprocessed_R1_D1-9-12_mid--AUDIO-from_mp3_12_R1_2015_wav--1.midi\n",
      "Processing file 6/13: MIDI-Unprocessed_Schubert10-12_MID--AUDIO_18_R2_2018_wav.midi\n",
      "Processing file 7/13: MIDI-UNPROCESSED_09-10_R1_2014_MID--AUDIO_09_R1_2014_wav--3.midi\n",
      "Processing file 8/13: MIDI-Unprocessed_01_R1_2009_01-04_ORIG_MID--AUDIO_01_R1_2009_01_R1_2009_01_WAV.midi\n",
      "Processing file 9/13: MIDI-Unprocessed_Schubert10-12_MID--AUDIO_20_R2_2018_wav.midi\n",
      "Processing file 10/13: MIDI-UNPROCESSED_09-10_R1_2014_MID--AUDIO_09_R1_2014_wav--2.midi\n",
      "Processing file 11/13: MIDI-Unprocessed_01_R1_2006_01-09_ORIG_MID--AUDIO_01_R1_2006_02_Track02_wav.midi\n",
      "Processing file 12/13: MIDI-Unprocessed_057_PIANO057_MID--AUDIO-split_07-07-17_Piano-e_1-07_wav--4.midi\n",
      "Processing file 13/13: MIDI-Unprocessed_01_R1_2006_01-09_ORIG_MID--AUDIO_01_R1_2006_01_Track01_wav.midi\n",
      "Successfully extracted 46059 notes and chords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 21:19:42.654932: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/var/data/python/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 21:19:43.974024: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 165782904 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 227ms/step - loss: 5.8189 - val_loss: 5.7630\n",
      "Epoch 2/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 225ms/step - loss: 5.4450 - val_loss: 5.7660\n",
      "Epoch 3/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 224ms/step - loss: 5.4427 - val_loss: 5.7500\n",
      "Epoch 4/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 224ms/step - loss: 5.4178 - val_loss: 5.7335\n",
      "Epoch 5/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 226ms/step - loss: 5.3417 - val_loss: 5.7579\n",
      "Epoch 6/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 225ms/step - loss: 5.3043 - val_loss: 5.7798\n",
      "Epoch 7/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 225ms/step - loss: 5.2931 - val_loss: 5.7514\n",
      "Epoch 8/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 223ms/step - loss: 5.2635 - val_loss: 5.7937\n",
      "Epoch 9/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 223ms/step - loss: 5.2443 - val_loss: 5.7624\n",
      "Epoch 10/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 224ms/step - loss: 5.2260 - val_loss: 5.7854\n",
      "Epoch 11/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 223ms/step - loss: 5.2229 - val_loss: 5.8170\n",
      "Epoch 12/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 222ms/step - loss: 5.2023 - val_loss: 5.7885\n",
      "Epoch 13/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 224ms/step - loss: 5.1915 - val_loss: 5.8196\n",
      "Epoch 14/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 223ms/step - loss: 5.1959 - val_loss: 5.8307\n",
      "Epoch 15/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 223ms/step - loss: 5.1713 - val_loss: 5.8404\n",
      "Epoch 16/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 223ms/step - loss: 5.1443 - val_loss: 5.8450\n",
      "Epoch 17/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 223ms/step - loss: 5.1380 - val_loss: 5.8858\n",
      "Epoch 18/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 224ms/step - loss: 5.1447 - val_loss: 5.8583\n",
      "Epoch 19/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 224ms/step - loss: 5.1199 - val_loss: 5.8788\n",
      "Epoch 20/20\n",
      "\u001b[1m324/324\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 223ms/step - loss: 5.1031 - val_loss: 5.9080\n",
      "Created piece with duration: 62.50 seconds\n",
      "Music generated and saved as 'generated_music2.mid'\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Load MIDI files\n",
    "    notes = load_midi_files(\"maestro-v3.0.0-midi/maestro-v3.0.0/Made up\")  # Replace with your MIDI file directory\n",
    "    n_vocab = len(set(notes))\n",
    "\n",
    "    # Prepare sequences\n",
    "    sequence_length = 100\n",
    "    network_input, network_output, note_to_int = prepare_sequences(notes, n_vocab, sequence_length)\n",
    "\n",
    "    # Build and train the model\n",
    "    model = build_model((network_input.shape[1], network_input.shape[2]), n_vocab)\n",
    "    model = train_model(model, network_input, network_output, epochs=20, batch_size=128)\n",
    "\n",
    "    # Generate music\n",
    "    pitchnames = sorted(set(notes))\n",
    "    prediction_output = generate_music(model, network_input, pitchnames, n_vocab, sequence_length, num_notes=500)\n",
    "\n",
    "    # Save the generated music as a MIDI file\n",
    "    create_midi(prediction_output, \"generated_music2.mid\")\n",
    "    print(\"Music generated and saved as 'generated_music2.mid'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
